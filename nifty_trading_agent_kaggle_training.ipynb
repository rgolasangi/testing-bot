{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nifty Trading Agent - Kaggle Notebook Training\n",
    "This notebook guides you through setting up the environment, accessing Kaggle datasets, and training the Reinforcement Learning (RL) agent.\n",
    "**Important**: Ensure you have a GPU runtime enabled for faster training. Go to `File -> Notebook settings` and select `GPU` as the accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone the GitHub Repository\n",
    "First, clone your project repository from GitHub. This will give you access to all the agent\"s code.\n",
    "**Note**: Kaggle Notebooks usually clone into `/kaggle/working/` by default. We will navigate into the cloned directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/rgolasangi/testing-bot.git\n",
    "%cd testing-bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "Install all the necessary Python libraries. This might take a few minutes.\n",
    "**Note**: If you encounter issues with `talib`, you might need to install `ta-lib` system-wide first or use a pre-built Docker image that includes it. For Kaggle, `pip install TA-Lib` usually works after installing `libta-lib-dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y libta-lib-dev\n",
    "!pip install -r nifty_trading_agent/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Access Kaggle Dataset\n",
    "The dataset `ayushsacri/indian-nifty-and-banknifty-options-data-2020-2024` is already available in Kaggle Notebooks. You can access it directly from the input directory.\n",
    "The path to the dataset will typically be `/kaggle/input/indian-nifty-and-banknifty-options-data-2020-2024/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "KAGGLE_DATA_PATH = \"/kaggle/input/indian-nifty-and-banknifty-options-data-2020-2024/\"\n",
    "print(f\"Kaggle dataset path: {KAGGLE_DATA_PATH}\")\n",
    "# List contents to verify\n",
    "!ls -F {KAGGLE_DATA_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Training\n",
    "We will now load the historical data from the Kaggle dataset. This data will be used for the initial training of the RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nifty_trading_agent.src.preprocessing.data_cleaner import DataCleaner\n",
    "from nifty_trading_agent.src.preprocessing.feature_engineer import FeatureEngineer\n",
    "\n",
    "data_cleaner = DataCleaner()\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "# Example: Load Nifty Spot data (adjust path as per actual dataset structure)\n",
    "# You might need to iterate through subdirectories or specific files\n",
    "# For demonstration, let\"s assume a single CSV for Nifty spot data exists.\n",
    "# You will need to adapt this part based on the actual structure of the Kaggle dataset.\n",
    "# For example, if data is split by year/month, you\"ll need to loop and concatenate.\n",
    "\n",
    "# Placeholder for loading data from Kaggle dataset\n",
    "# This part needs to be adapted based on the actual file structure within the Kaggle dataset.\n",
    "# For instance, if it\"s a collection of CSVs, you\"ll need to read them all and concatenate.\n",
    "# Example: Reading a single Nifty spot CSV (adjust path as needed)\n",
    "try:\n",
    "    nifty_spot_df = pd.read_csv(os.path.join(KAGGLE_DATA_PATH, \"nifty_data/nifty_spot/2023/1/nifty_spot01_01_2023.csv\")) # Example path\n",
    "    print(\"Loaded Nifty Spot Data.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Nifty spot data not found at example path. Please adjust the path based on the dataset structure.\")\n",
    "    nifty_spot_df = pd.DataFrame() # Empty DataFrame if not found\n",
    "\n",
    "if not nifty_spot_df.empty:\n",
    "    # Clean and engineer features\n",
    "    nifty_spot_df = data_cleaner.clean_market_data(nifty_spot_df)\n",
    "    nifty_spot_df = feature_engineer.add_technical_indicators(nifty_spot_df)\n",
    "    nifty_spot_df = feature_engineer.add_volatility_features(nifty_spot_df)\n",
    "    # Add other features as needed, e.g., options features if available in the dataset\n",
    "    print(\"Data cleaned and features engineered.\")\n",
    "else:\n",
    "    print(\"Skipping data cleaning and feature engineering due to empty DataFrame.\")\n",
    "\n",
    "# The processed DataFrame (nifty_spot_df) can now be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Securely Configure Zerodha API Keys (Kaggle Secrets)\n",
    "While initial training uses Kaggle data, for fine-tuning with live data or for the `live_data_trainer` to connect to Zerodha, you will need to configure your Zerodha API keys.\n",
    "Kaggle Notebooks provide a secure way to store secrets:\n",
    "1.  Go to `Add-ons -> Secrets` in the notebook menu.\n",
    "2.  Add the following secrets with your actual values:\n",
    "    *   `ZERODHA_API_KEY`\n",
    "    *   `ZERODHA_API_SECRET`\n",
    "    *   `ZERODHA_ACCESS_TOKEN` (If you already have one, otherwise the trainer will guide you to generate it)\n",
    "3.  Ensure the secrets are \"Attached\" to this notebook.\n",
    "\n",
    "After adding, run the cell below to load them as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ[\"ZERODHA_API_KEY\"] = user_secrets.get_secret(\"ZERODHA_API_KEY\")\n",
    "os.environ[\"ZERODHA_API_SECRET\"] = user_secrets.get_secret(\"ZERODHA_API_SECRET\")\n",
    "try:\n",
    "    os.environ[\"ZERODHA_ACCESS_TOKEN\"] = user_secrets.get_secret(\"ZERODHA_ACCESS_TOKEN\")\n",
    "except: # Handle case where ACCESS_TOKEN might not be set initially\n",
    "    pass\n",
    "\n",
    "print(\"Zerodha API keys loaded from Kaggle Secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the RL Agent Training\n",
    "Now, you can run the training script. This script will:\n",
    "*   Use the prepared Kaggle historical data for initial training.\n",
    "*   (Optional) Connect to Zerodha for live data fine-tuning if configured.\n",
    "*   Save trained models to a persistent location (e.g., `/kaggle/working/models/` which can be downloaded or linked to Google Drive/GCS).\n",
    "\n",
    "**Note**: The training process can be long. You can monitor the progress and logs here.\n",
    "**To stop training**: Interrupt the execution of this cell (Runtime -> Interrupt execution or Ctrl+M I)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the current working directory is the root of the cloned repository\n",
    "import os\n",
    "os.chdir(\"/kaggle/working/testing-bot\")\n",
    "\n",
    "# Set the model save path to a Kaggle persistent directory\n",
    "MODEL_SAVE_DIR = \"/kaggle/working/models/\"\n",
    "!mkdir -p {MODEL_SAVE_DIR}\n",
    "\n",
    "from nifty_trading_agent.src.utils.config_manager import config\n",
    "config.set(\'rl_agent.model_save_path\', MODEL_SAVE_DIR)\n",
    "\n",
    "# Run the trainer script. You might need to modify live_data_trainer.py\n",
    "# to accept a historical data path instead of always fetching from Zerodha.\n",
    "# For now, it will attempt to use Zerodha if credentials are set.\n",
    "# If you want to train purely on Kaggle data without Zerodha, you\"ll need to adapt\n",
    "# the live_data_trainer.py or create a new training script.\n",
    "trainer = LiveDataTrainer()\n",
    "trainer.run_trainer(historical_data_dir=KAGGLE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Trained Models\n",
    "After training, the models will be saved in the `{MODEL_SAVE_DIR}` directory. You can download them from Kaggle or integrate with Google Drive/GCS for persistent storage if needed."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}


